{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b0000001",
   "metadata": {},
   "source": [
    "# 1b. Export RT-IGEV++ to ONNX\n",
    "\n",
    "Converts an **RT-IGEV++** (real-time variant) PyTorch checkpoint to an\n",
    "ONNX model that can be loaded by\n",
    "`ag-cam-tools depth-preview --stereo-backend onnx`.\n",
    "\n",
    "RT-IGEV++ is significantly faster than the full IGEV++ model while\n",
    "maintaining strong accuracy. Key simplifications:\n",
    "\n",
    "- Single shared backbone (no separate context network)\n",
    "- Single-range geometry encoding volume (vs. multi-range)\n",
    "- Single-level ConvGRU with 96 hidden channels (vs. 3-level with 128)\n",
    "- Default `max_disp=192`, `iters=8`\n",
    "\n",
    "The exported model expects two float32 inputs of shape `[1, 3, H, W]` in\n",
    "`[0, 255]` range and produces a float32 disparity map.\n",
    "\n",
    "**This export is tied to a specific resolution and max_disp.** If you\n",
    "recalibrate or change binning, re-run this notebook.\n",
    "\n",
    "---\n",
    "\n",
    "### Prerequisites\n",
    "\n",
    "1. Clone the IGEV++ repository:\n",
    "   ```bash\n",
    "   git clone https://github.com/gangweiX/IGEV-plusplus.git ~/code/IGEV-plusplus\n",
    "   ```\n",
    "\n",
    "2. Download the RT-IGEV++ SceneFlow checkpoint from the\n",
    "   [Google Drive folder](https://drive.google.com/drive/folders/1eubNsu03MlhUfTtrbtN7bfAsl39s2ywJ):\n",
    "   ```bash\n",
    "   mkdir -p ~/code/IGEV-plusplus/pretrained_models/rt_igev_plusplus\n",
    "   # Download sceneflow.pth into that directory.\n",
    "   ```\n",
    "\n",
    "3. Have a completed calibration session (from `ag-cam-tools calibration-capture`\n",
    "   + `2.Calibration.ipynb`).\n",
    "\n",
    "### Setup (one-time)\n",
    "```bash\n",
    "cd backends\n",
    "python3 -m venv .venv\n",
    "source .venv/bin/activate\n",
    "pip install -r requirements-onnx-export.txt\n",
    "python -m ipykernel install --user --name agrippa-export --display-name \"agrippa-export\"\n",
    "```\n",
    "\n",
    "Then select the **agrippa-export** kernel in Jupyter before running."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0000002",
   "metadata": {},
   "source": [
    "## Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0000003",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -r requirements-onnx-export.txt -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0000004",
   "metadata": {},
   "outputs": [],
   "source": [
    "import contextlib\n",
    "import json\n",
    "import os\n",
    "import struct\n",
    "import sys\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0000005",
   "metadata": {},
   "source": [
    "## Configuration\n",
    "\n",
    "Set these paths to match your setup.  `calibration_session` should point\n",
    "to a calibrated session folder containing `calib_result/calibration_meta.json`\n",
    "and `calib_result/remap_left.bin`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0000006",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to cloned IGEV-plusplus repo root (contains core_rt/).\n",
    "igev_src = os.path.expanduser(\"~/code/IGEV-plusplus\")\n",
    "\n",
    "# Path to pre-trained RT-IGEV++ checkpoint.\n",
    "checkpoint_path = os.path.join(\n",
    "    igev_src, \"pretrained_models/rt_igev_plusplus/sceneflow.pth\")\n",
    "\n",
    "# Calibration session folder.\n",
    "calibration_session = \"../calibration/calibration_20260225_160029_3cb907d2\"\n",
    "\n",
    "# GRU refinement iterations (8 is the upstream default for RT-IGEV++).\n",
    "iters = 8\n",
    "\n",
    "# Output ONNX file path.\n",
    "output_path = \"../models/rt_igev_plusplus.onnx\"\n",
    "\n",
    "# Optional overrides (set to None to auto-detect from calibration).\n",
    "override_max_disp = None   # e.g. 192\n",
    "override_width    = None   # e.g. 720\n",
    "override_height   = None   # e.g. 540"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0000007",
   "metadata": {},
   "source": [
    "## Load calibration metadata\n",
    "\n",
    "Reads `calibration_meta.json` to derive `max_disp` (cost volume size)\n",
    "and the remap table header to get the image dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0000008",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Read max_disp from calibration_meta.json ---\n",
    "meta_path = os.path.join(calibration_session, \"calib_result\",\n",
    "                         \"calibration_meta.json\")\n",
    "with open(meta_path, \"r\") as f:\n",
    "    meta = json.load(f)\n",
    "\n",
    "dr = meta.get(\"disparity_range\", {})\n",
    "num_disparities = dr.get(\"num_disparities\", 128)\n",
    "\n",
    "# RT-IGEV++ uses a single-range cost volume with D = max_disp // 4.\n",
    "# The 3D hourglass has 3 stride-2 down/up stages, so D must be\n",
    "# divisible by 8.  This means max_disp must be divisible by 32.\n",
    "#\n",
    "# The upstream default is 192 (D=48, 48/8=6).\n",
    "# We round up to the next multiple of 32 that covers num_disparities.\n",
    "raw = override_max_disp or max(num_disparities, 192)\n",
    "max_disp = ((raw + 31) // 32) * 32\n",
    "\n",
    "print(f\"max_disp = {max_disp}  (calibration num_disparities = {num_disparities})\")\n",
    "\n",
    "# --- Read image dimensions from remap table header ---\n",
    "# Binary format: \"RMAP\" (4 bytes) + width(u32) + height(u32) + flags(u32)\n",
    "width = override_width\n",
    "height = override_height\n",
    "\n",
    "if width is None or height is None:\n",
    "    remap_path = os.path.join(calibration_session, \"calib_result\",\n",
    "                              \"remap_left.bin\")\n",
    "    with open(remap_path, \"rb\") as f:\n",
    "        magic = f.read(4)\n",
    "        assert magic == b\"RMAP\", f\"Bad remap magic: {magic!r}\"\n",
    "        w, h, _flags = struct.unpack(\"<III\", f.read(12))\n",
    "        if width is None:\n",
    "            width = w\n",
    "        if height is None:\n",
    "            height = h\n",
    "\n",
    "def pad_to_32(dim):\n",
    "    \"\"\"Round up to nearest multiple of 32.\"\"\"\n",
    "    return ((dim + 31) // 32) * 32\n",
    "\n",
    "pad_w = pad_to_32(width)\n",
    "pad_h = pad_to_32(height)\n",
    "\n",
    "print(f\"Frame dimensions: {width}x{height}\")\n",
    "print(f\"Padded (32-divisible): {pad_w}x{pad_h}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0000009",
   "metadata": {},
   "source": [
    "## Patch autocast for CPU tracing\n",
    "\n",
    "RT-IGEV++ uses `torch.cuda.amp.autocast` in its forward pass and in\n",
    "`core_rt/update.py`.  We replace it with a no-op context manager so\n",
    "ONNX tracing works on CPU-only machines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0000010",
   "metadata": {},
   "outputs": [],
   "source": [
    "class _NoOpAutocast:\n",
    "    \"\"\"Drop-in replacement for torch.cuda.amp.autocast.\"\"\"\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        pass\n",
    "    def __enter__(self):\n",
    "        return self\n",
    "    def __exit__(self, *args):\n",
    "        pass\n",
    "    def __call__(self, func):\n",
    "        return func\n",
    "\n",
    "if hasattr(torch.cuda, \"amp\"):\n",
    "    torch.cuda.amp.autocast = _NoOpAutocast\n",
    "\n",
    "if hasattr(torch, \"autocast\"):\n",
    "    _orig_autocast = torch.autocast\n",
    "    def _patched_autocast(device_type=\"cuda\", *args, **kwargs):\n",
    "        if device_type == \"cuda\":\n",
    "            return contextlib.nullcontext()\n",
    "        return _orig_autocast(device_type, *args, **kwargs)\n",
    "    torch.autocast = _patched_autocast\n",
    "\n",
    "print(\"Patched autocast for CPU tracing.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0000011",
   "metadata": {},
   "source": [
    "## Load RT-IGEV++ model\n",
    "\n",
    "Imports the RT-IGEV++ model from `core_rt/`, builds the configuration\n",
    "args it expects, and loads the pre-trained checkpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0000012",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add IGEV++ source to path.\n",
    "sys.path.insert(0, igev_src)\n",
    "\n",
    "from core_rt.rt_igev_stereo import IGEVStereo\n",
    "print(\"Imported IGEVStereo from core_rt.rt_igev_stereo\")\n",
    "\n",
    "# Build args namespace that RT-IGEV++ expects.\n",
    "# Much simpler than full IGEV++ — single-range volume, scalar hidden_dim.\n",
    "class ModelArgs:\n",
    "    pass\n",
    "\n",
    "args = ModelArgs()\n",
    "args.max_disp = max_disp\n",
    "args.valid_iters = iters\n",
    "args.mixed_precision = False\n",
    "args.precision_dtype = \"float32\"\n",
    "args.corr_levels = 2\n",
    "args.corr_radius = 4\n",
    "args.n_downsample = 2\n",
    "args.n_gru_layers = 3\n",
    "args.hidden_dim = 96\n",
    "\n",
    "model = IGEVStereo(args)\n",
    "print(f\"Model instantiated (max_disp={max_disp}, hidden_dim=96, iters={iters})\")\n",
    "\n",
    "# Load checkpoint.\n",
    "# weights_only=False is required — the upstream checkpoint was saved with\n",
    "# older PyTorch and contains objects that the safe unpickler rejects.\n",
    "print(f\"Loading checkpoint: {checkpoint_path}\")\n",
    "state_dict = torch.load(checkpoint_path, map_location=\"cpu\", weights_only=False)\n",
    "\n",
    "# Handle DataParallel wrapper prefix.\n",
    "if any(k.startswith(\"module.\") for k in state_dict.keys()):\n",
    "    state_dict = {k.replace(\"module.\", \"\"): v\n",
    "                  for k, v in state_dict.items()}\n",
    "    print(\"Stripped 'module.' prefix from state_dict.\")\n",
    "\n",
    "model.load_state_dict(state_dict, strict=False)\n",
    "model.eval()\n",
    "print(\"Checkpoint loaded.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0000013",
   "metadata": {},
   "source": [
    "## Export to ONNX\n",
    "\n",
    "Traces the model at the padded resolution with `torch.onnx.export` at\n",
    "opset 16.  The dummy inputs use `[0, 255]` float32 range (not `[0, 1]`)\n",
    "which matches what the C backend will send."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0000014",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure output directory exists.\n",
    "out_dir = os.path.dirname(output_path)\n",
    "if out_dir:\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "# RT-IGEV++ expects [0, 255] float32 input.\n",
    "left_dummy = torch.randn(1, 3, pad_h, pad_w) * 128.0 + 128.0\n",
    "right_dummy = torch.randn(1, 3, pad_h, pad_w) * 128.0 + 128.0\n",
    "\n",
    "# Set test_mode to get single output (final refinement only).\n",
    "model.test_mode = True\n",
    "\n",
    "print(f\"Tracing with opset 16 at {pad_w}x{pad_h} (iters={iters})...\")\n",
    "t0 = time.time()\n",
    "\n",
    "with torch.no_grad():\n",
    "    torch.onnx.export(\n",
    "        model,\n",
    "        (left_dummy, right_dummy),\n",
    "        output_path,\n",
    "        opset_version=16,\n",
    "        input_names=[\"left\", \"right\"],\n",
    "        output_names=[\"disparity\"],\n",
    "        dynamic_axes=None,  # fixed resolution for best performance\n",
    "        do_constant_folding=True,\n",
    "        dynamo=False,  # force legacy JIT-trace exporter (dynamo can't handle IGEV++)\n",
    "    )\n",
    "\n",
    "dt = time.time() - t0\n",
    "size_mb = os.path.getsize(output_path) / (1024 * 1024)\n",
    "print(f\"Raw ONNX exported in {dt:.1f}s ({size_mb:.1f} MB)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0000015",
   "metadata": {},
   "source": [
    "## Simplify with onnxsim\n",
    "\n",
    "Runs graph optimizations (constant folding, shape inference, dead-node\n",
    "elimination) to reduce model size and improve inference speed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0000016",
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnx\n",
    "from onnxsim import simplify\n",
    "\n",
    "print(\"Simplifying with onnxsim...\")\n",
    "t0 = time.time()\n",
    "\n",
    "onnx_model = onnx.load(output_path)\n",
    "model_sim, check = simplify(onnx_model)\n",
    "\n",
    "if check:\n",
    "    onnx.save(model_sim, output_path)\n",
    "    dt = time.time() - t0\n",
    "    size_mb = os.path.getsize(output_path) / (1024 * 1024)\n",
    "    print(f\"Simplified in {dt:.1f}s ({size_mb:.1f} MB)\")\n",
    "else:\n",
    "    print(\"onnxsim check failed, keeping unsimplified model.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0000017",
   "metadata": {},
   "source": [
    "## Validate with ONNX Runtime\n",
    "\n",
    "Loads the exported model in ONNX Runtime and runs a single inference pass\n",
    "to verify the model is valid and produces reasonable output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0000018",
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnxruntime as ort\n",
    "\n",
    "session = ort.InferenceSession(output_path,\n",
    "                               providers=[\"CPUExecutionProvider\"])\n",
    "\n",
    "inputs = session.get_inputs()\n",
    "outputs = session.get_outputs()\n",
    "print(f\"Inputs:  {[(i.name, i.shape, i.type) for i in inputs]}\")\n",
    "print(f\"Outputs: {[(o.name, o.shape, o.type) for o in outputs]}\")\n",
    "\n",
    "# Warm-up / validation inference.\n",
    "left = np.random.uniform(0, 255, (1, 3, pad_h, pad_w)).astype(np.float32)\n",
    "right = np.random.uniform(0, 255, (1, 3, pad_h, pad_w)).astype(np.float32)\n",
    "\n",
    "t0 = time.time()\n",
    "results = session.run(None, {inputs[0].name: left, inputs[1].name: right})\n",
    "dt = time.time() - t0\n",
    "\n",
    "disp = results[-1].squeeze()\n",
    "print(f\"\\nInference time: {dt:.2f}s\")\n",
    "print(f\"Output shape: {disp.shape}\")\n",
    "print(f\"Disparity range: [{disp.min():.2f}, {disp.max():.2f}]\")\n",
    "\n",
    "if disp.shape != (pad_h, pad_w):\n",
    "    print(f\"\\nWARNING: output shape {disp.shape} != expected ({pad_h}, {pad_w})\")\n",
    "else:\n",
    "    print(\"\\nValidation OK.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0000019",
   "metadata": {},
   "source": [
    "## Done\n",
    "\n",
    "The RT-IGEV++ ONNX model has been exported, simplified, and validated.\n",
    "\n",
    "### Next steps\n",
    "\n",
    "1. Copy the model to your target machine:\n",
    "   ```bash\n",
    "   scp models/rt_igev_plusplus.onnx jetson:~/agrippa-stereocam/models/\n",
    "   ```\n",
    "\n",
    "2. Run depth preview:\n",
    "   ```bash\n",
    "   ag-cam-tools depth-preview \\\n",
    "       --rectify calibration/calibration_YYYYMMDD_HHMMSS \\\n",
    "       --stereo-backend onnx \\\n",
    "       --model-path models/rt_igev_plusplus.onnx\n",
    "   ```\n",
    "\n",
    "The C backend automatically selects the best execution provider\n",
    "(CUDA > CPU) and handles padding/preprocessing."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "agrippa-export",
   "language": "python",
   "name": "agrippa-export"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}